---
title: "Practical Machine Learning Week 4 - Assignment"
author: "Mohankumar Subbiah"
date: "September 3, 2017"
output: html_document
---

## Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.

For more details, please visit: http://groupware.les.inf.puc-rio.br/har

The goal of your project is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

```
library(rpart)
library(knitr)
library(caret)
library(corrplot)
library(dplyr)
library(rpart.plot)
library(randomForest)
library(rattle)
set.seed(2017-01-06)
```

Download the files from the websites supplied:

Read and clean the datas:

```
# training is for the real train and test sets.

training <- read.table(file = "pml-training.csv", header = T, sep = ",")

# test is for the 20 quiz questions.
test <- read.table(file = "pml-testing.csv", header = T, sep = ",")

# Some of the variables are flat without much variance, which is uselessful for correlation detection. So we are going to remove them.

flat <- nearZeroVar(training)
training <- training[,-flat]

# There are many variables containing NA, which is annoying for later modelling. We are going to remove the variables that are mostly NAs.

good <- sapply(training,function(x) mean(is.na(x))) <=0.95
training <- training[,good]

# some of the variables are labeles or identification of the observations. They have nothing to do with the correlation or modelling. We will pick them off the data set.

training <- training[,-c(1:5)]

# Now, the data is clean. We want to randomly split the data to trainSet and testSet.

inTrain<- createDataPartition(training$classe,p=0.7,list = F)
trainSet <- training[inTrain,]
testSet <- training[-inTrain,]
```
Explore the data (roughly check out the correlation among the variables, make sure there are no highly correlated variables to slow down the modelling algorithms):

```
# The correlation numbers will be shown in a more eligible way by recruiting corrplot function. The darker red and blue indicates the higher correlation.

corGraph <- cor(trainSet[, colnames(trainSet)!="classe"])
corrplot(corGraph, order = "FPC", method = "number", type = "lower", 
         tl.cex = 0.8, tl.col = rgb(0, 0, 0),number.cex = 0.7, number.digits = 2)
```
Plot1![Rplot 1](pml_plot/Rplot1.png) 

There are several variables that are highly correlated (>0.90 or < -0.90). In order to save the calculating source, we are going to remove some of the variables.

```
# The redundant variables have been detected according to the correlation number shown above. 

Those variables which share over 0.90 (or less than -0.90) correlation will be removed and left only one variable.

trainSet <- trainSet %>%
        select(-c(accel_belt_y,roll_belt,accel_belt_z,gyros_arm_y,pitch_belt,gyros_dumbbell_z,gyros_dumbbell_x,gyros_forearm_z))

testSet <- testSet %>%
        select(-c(accel_belt_y,roll_belt,accel_belt_z,gyros_arm_y,pitch_belt,gyros_dumbbell_z,gyros_dumbbell_x,gyros_forearm_z))
```

## Modelling and prediction:

In order to obtain the best modelling, we are going to try three different methods (Generalized Boosted Model, Random Forest, Decison Tree) and pick up the one with most accurate prediction rate by predicting and comparing with the testSet "classe", using the three modelling respectively.

### This is first modelling with methods of "gbm":

```
# This is the 1st model by using the gbm methods.

set.seed(2017-01-04)
controlGBM <- trainControl(method = "repeatedcv", number = 5, repeats = 1)
mod_gbm <- train(classe ~ ., data=trainSet, method = "gbm",trControl=controlGBM,verbose=FALSE)
mod_gbm$finalModel

```
### Output:
```
## A gradient boosted model with multinomial loss function.
## 150 iterations were performed.
## There were 45 predictors of which 39 had non-zero influence.
```

```
# Predict the results by using the testSet. And calculate the accuracy by comparing with the testSet "classe".

pred_gbm <- predict(mod_gbm, newdata=testSet)
conf_gbm <- confusionMatrix(pred_gbm, testSet$classe)
conf_gbm
```
### Output:
```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1672   10    0    0    0
         B    2 1123   12    3    3
         C    0    6 1011   12    3
         D    0    0    3  946    9
         E    0    0    0    3 1067

Overall Statistics
                                          
               Accuracy : 0.9888          
                 95% CI : (0.9858, 0.9913)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9858          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9988   0.9860   0.9854   0.9813   0.9861
Specificity            0.9976   0.9958   0.9957   0.9976   0.9994
Pos Pred Value         0.9941   0.9825   0.9797   0.9875   0.9972
Neg Pred Value         0.9995   0.9966   0.9969   0.9963   0.9969
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2841   0.1908   0.1718   0.1607   0.1813
Detection Prevalence   0.2858   0.1942   0.1754   0.1628   0.1818
Balanced Accuracy      0.9982   0.9909   0.9905   0.9894   0.9928

```

```
# Plot the prediction restuls.

plot(conf_gbm$table, col = conf_gbm$byClass, 
     main = paste("Accuracy of gbm is ", round(conf_gbm$overall['Accuracy'], 4)))

```
Plot2![Rplot 2](pml_plot/Rplot2.png)

### This is the second modelling with the methods of "rf":

```
# This is the modelling with the methods of random forest:

set.seed(2017-01-04)
controlRF <- trainControl(method="cv", number=3, verboseIter=FALSE)
mod_rf <- train(classe ~ ., data=trainSet, method="rf", trControl=controlRF)
mod_rf$finalModel

```
### Output:
```
Call:
 randomForest(x = x, y = y, mtry = param$mtry) 
               Type of random forest: classification
                     Number of trees: 500
No. of variables tried at each split: 23

        OOB estimate of  error rate: 0.23%
Confusion matrix:
     A    B    C    D    E  class.error
A 3904    1    0    0    1 0.0005120328
B    4 2646    8    0    0 0.0045146727
C    0   10 2386    0    0 0.0041736227
D    0    0    6 2246    0 0.0026642984
E    0    0    0    2 2523 0.0007920792


```
```
# Predict the results with the testSet and compare with the testSet "classe"

pred_rf <- predict(mod_rf, newdata=testSet)
conf_rf <- confusionMatrix(pred_rf, testSet$classe)
conf_rf

```
### Output:
```
Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1674    6    0    0    0
         B    0 1127    0    0    0
         C    0    6 1026   10    0
         D    0    0    0  953    3
         E    0    0    0    1 1079

Overall Statistics
                                          
               Accuracy : 0.9956          
                 95% CI : (0.9935, 0.9971)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.9944          
 Mcnemar's Test P-Value : NA              

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            1.0000   0.9895   1.0000   0.9886   0.9972
Specificity            0.9986   1.0000   0.9967   0.9994   0.9998
Pos Pred Value         0.9964   1.0000   0.9846   0.9969   0.9991
Neg Pred Value         1.0000   0.9975   1.0000   0.9978   0.9994
Prevalence             0.2845   0.1935   0.1743   0.1638   0.1839
Detection Rate         0.2845   0.1915   0.1743   0.1619   0.1833
Detection Prevalence   0.2855   0.1915   0.1771   0.1624   0.1835
Balanced Accuracy      0.9993   0.9947   0.9984   0.9940   0.9985

```

```
# plot the predict result

plot(conf_rf$table, col = conf_rf$byClass, 
     main = paste("Accuracy of the random forest is ",
                  round(conf_rf$overall['Accuracy'], 4)))

```
Plot3![Rplot 3](pml_plot/Rplot3.png)



This is the third modelling with the methods of "Decision Tree":

```
# Model with the methods of "decision tree"

set.seed(2017-01-04)
mod_dtr <- rpart(classe ~ ., data=trainSet, method="class")
fancyRpartPlot(mod_dtr)

```
Plot4![Rplot 4](pml_plot/Rplot4.png)


```
# predict the results with the data testSet

pred_dtr <- predict(mod_dtr, newdata=testSet, type="class")
conf_dtr <- confusionMatrix(pred_dtr, testSet$classe)
conf_dtr

```
### Output:
```

Confusion Matrix and Statistics

          Reference
Prediction    A    B    C    D    E
         A 1538  162   48   71   84
         B   54  680  119  205  180
         C   35  190  789  119  138
         D   43   76   30  523  100
         E    4   31   40   46  580

Overall Statistics
                                          
               Accuracy : 0.6984          
                 95% CI : (0.6865, 0.7101)
    No Information Rate : 0.2845          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.6164          
 Mcnemar's Test P-Value : < 2.2e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9188   0.5970   0.7690  0.54253  0.53604
Specificity            0.9133   0.8824   0.9008  0.94940  0.97481
Pos Pred Value         0.8082   0.5493   0.6208  0.67746  0.82739
Neg Pred Value         0.9658   0.9012   0.9486  0.91375  0.90316
Prevalence             0.2845   0.1935   0.1743  0.16381  0.18386
Detection Rate         0.2613   0.1155   0.1341  0.08887  0.09856
Detection Prevalence   0.3234   0.2104   0.2160  0.13118  0.11912
Balanced Accuracy      0.9160   0.7397   0.8349  0.74597  0.75543

```

```
# plot the prediction results

plot(conf_dtr$table, col = conf_dtr$byClass, 
     main = paste("Accuracy of the decison tree is ",
                  round(conf_dtr$overall['Accuracy'], 4)))

```
Plot5![Rplot 5](pml_plot/Rplot5.png)

### Conclusion:

According to the modelling of each methods, the accuracies are: Generalized boosted Models : 0.9886 Random Forest: 0.9959 Decision Tree: 0.6984 So, the Random Forest model is most accurate. The Generalized boosted model is second best one. Decision Tree is worst and not reliable.

### For the 20 quiz test questions:

The object "test" contains the data which is used for the practice:

```
quiz <- predict(mod_rf, newdata=test)
quiz

```

```

[1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E

```
